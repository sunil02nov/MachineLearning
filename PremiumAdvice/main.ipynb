{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis object initialised\n",
      "Data_Availability_Report created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shraddha.sharma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\shraddha.sharma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\shraddha.sharma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data transformation object initialised\n",
      "['principal component 1', 'General Liability Allocation_l', 'Coverage Premium_l', 'Attachment Amount_l', 'Terrorism_l', 'Auto Liability Allocation_l', 'BusinessClassification', 'InsuredState', 'duration']\n",
      "Total rows:  36021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shraddha.sharma\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from AnalysisModule import data_analysis\n",
    "from TransformationModule import data_transformation\n",
    "import SettingsModule as sm\n",
    "import pandas as pd\n",
    "from time import strftime, localtime\n",
    "import logging,time,json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency,chisquare\n",
    "import pandas, copy, pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(filename=sm.LOG_FILE,level=logging.DEBUG)\n",
    "    da = data_analysis()\n",
    "    \n",
    "    data_path = sm.DATA_FILE_PATH + sm.DATA_FILE_NAME\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Loading data from location : ' + data_path)\n",
    "    da.get_data(data_path)\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Data Loading complete.')\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Memory Usage : ' \n",
    "                 + str(da.data.memory_usage(index=True,deep=True).sum()/1000000) + 'KB')\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Data Description below')\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Columns : ' + str(da.data.shape[0]))\n",
    "    logging.info(strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()) + '\\tInfo: Rows : ' + str(da.data.shape[1]))\n",
    "    \n",
    "    da.get_column_details().to_csv(sm.DOCUMENT_FOLDER + '\\Data_Availability_Report.csv')\n",
    "    print('Data_Availability_Report created')\n",
    "    \n",
    "    datatypes_json = sm.DATATYPE\n",
    "    split_data = da.split_data_on_datatypes(datatypes_json)\n",
    "    \n",
    "    dt = data_transformation()\n",
    "    \n",
    "    for each in split_data['numerical']:\n",
    "        y = 0\n",
    "        split_data['numerical'][each] = split_data['numerical'][each].apply(lambda x: dt.impute(x,y))\n",
    "        \n",
    "    for each in split_data['categorical']:\n",
    "        split_data['categorical'][each] = split_data['categorical'][each].apply(lambda x: dt.impute(x,'unknown'))\n",
    "\n",
    "    features = ['Layer Aggregate_l', 'Total Layer Limit_l', 'Per Occurrence limit_l','Aggregate_l']\n",
    "\n",
    "    # Separating out the features\n",
    "    df = split_data['numerical']\n",
    "    x = df[features]\n",
    "\n",
    "    # Standardizing the features\n",
    "#     std_scaler = StandardScaler().fit(x)\n",
    "#     pickle.dump( std_scaler, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\std_scaler.pkl', \"wb\" ) )\n",
    "    std_scale = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\std_scaler.pkl', \"rb\" ) )\n",
    "    x = std_scale.transform(x)\n",
    "\n",
    "    # PCA\n",
    "    \n",
    "#     pca = PCA(n_components=1)\n",
    "#     principalComponents = pca.fit(x)\n",
    "#     pickle.dump( principalComponents, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\pca.pkl', \"wb\" ) )\n",
    "    principalComponent = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\pca.pkl', \"rb\" ) )\n",
    "    principalComponents = principalComponent.transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1'])\n",
    "    \n",
    "    # merging PCA and other numerical columns\n",
    "\n",
    "    df.drop(features, axis=1, inplace=True)\n",
    "    num_data = pd.concat([principalDf,df],axis=1)\n",
    "    \n",
    "    y_cat =  split_data['categorical']['status']\n",
    "\n",
    "    ## label encoding categorical variable\n",
    "    df =pd.DataFrame()\n",
    "    for each in split_data['categorical'].columns:\n",
    "#         label_encoder = LabelEncoder()\n",
    "#         enc = label_encoder.fit(split_data['categorical'][each])\n",
    "#         pickle.dump( enc, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\encoder_' +  str(each.replace('/','_')) +'.pkl', \"wb\" ) )\n",
    "        encoder = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\encoder_' +  str(each.replace('/','_')) +'.pkl', \"rb\" ) )\n",
    "        enc = encoder.transform(split_data['categorical'][each])\n",
    "        df = pd.concat([df,pd.DataFrame(enc)],axis=1)\n",
    "    df.columns = split_data['categorical'].columns\n",
    "    y_num = df['status']\n",
    "    df.drop('Line of Business',axis = 1,inplace = True) # removing Line of Business because there is only one Line of Business\n",
    "    \n",
    "#     def chisq_of_df_cols(df, c1, c2):\n",
    "#         groupsizes = df.groupby([c1, c2]).size()\n",
    "#         ctsum = groupsizes.unstack(c1)\n",
    "#         # fillna(0) is necessary to remove any NAs which will cause exceptions\n",
    "#         return(chi2_contingency(ctsum.fillna(0)))\n",
    "\n",
    "#     mtx = pd.DataFrame(columns = df.columns, index =df.columns)\n",
    "\n",
    "#     for col1 in df.columns:\n",
    "#         for col2 in df.columns:\n",
    "#             if col1 == col2:\n",
    "#                 mtx[col1][col2] = 1\n",
    "#             else:\n",
    "#                 mtx[col1][col2] = chisq_of_df_cols(df, col1, col2)[1]\n",
    "                \n",
    "    # Win score\n",
    "\n",
    "    def f1(x,y):\n",
    "        return (x-y).days\n",
    "\n",
    "    duration = split_data['datetime'].apply(lambda x:f1(x['ExpiryDate'],x['InceptionDate']),axis=1)\n",
    "    cols = list(num_data.columns.values)\n",
    "    cols.extend(split_data['categorical'].columns.values)\n",
    "    cols.remove('Line of Business')\n",
    "    cols.append('duration')\n",
    "    cols.remove('status')\n",
    "    df1 = pd.concat([num_data,df,duration,y_num],axis = 1)\n",
    "    df1['status_cat'] = y_cat\n",
    "    X = df1[df1['Coverage Premium_l'] > 0]\n",
    "    y = X['status_cat']\n",
    "    X = X.drop('status',axis=1)\n",
    "    X = X.drop('status_cat',axis=1)\n",
    "    X.columns = cols\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)\n",
    "    \n",
    "    # feature importance and top imp features\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    temp = pd.DataFrame([X.columns.values,clf.feature_importances_],index = ['columns','imp']).T\n",
    "    imp_col = list(temp[temp['imp'] > 0.009]['columns'])\n",
    "    print(imp_col)\n",
    "    \n",
    "#     clf = GradientBoostingClassifier()\n",
    "#     clf.fit(X[imp_col], y)\n",
    "#     pickle.dump( clf, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\model\\model.pkl', \"wb\" ) )\n",
    "    model = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\model\\model.pkl', \"rb\" ) )\n",
    "        \n",
    "    y_pred_prob = pd.DataFrame(model.predict_proba(X[imp_col]),columns = model.classes_)\n",
    "    \n",
    "    def get_score(prob):\n",
    "        score = (((((prob['Bound'] + prob['Issued']+prob['Cleared']+prob['Outstanding Quote'])-(prob['Cancelled']+prob['Dead']+prob['Declined']+prob['Terminated']+prob['Void']))+1)/2)*0.9 + 0.05)*100\n",
    "        return score\n",
    "    print('Total rows: ',len(y_pred_prob))\n",
    "    y_pred_prob['Score'] = None\n",
    "    for i in range(len(y_pred_prob)):\n",
    "        y_pred_prob['Score'][i] = get_score(y_pred_prob[i:i+1])[i]\n",
    "        \n",
    "    df_prem = pd.concat([df[:36020],num_data[:36020],duration[:36020],y_pred_prob['Score'][:36020]],axis = 1)\n",
    "    \n",
    "    # outlier analysis\n",
    "    PREDICTED_COL = 'Coverage Premium_l'\n",
    "\n",
    "    X_col =  copy.deepcopy(list(df_prem.columns.values))\n",
    "    X_col.remove(PREDICTED_COL)\n",
    "\n",
    "    isof = IsolationForest()\n",
    "    isof.fit(df_prem)\n",
    "    df_prem['Outlier'] = isof.predict(df_prem)\n",
    "    \n",
    "    df_prem_no_outlier = df_prem[df_prem['Outlier'] == 1]\n",
    "    X = df_prem_no_outlier[X_col]\n",
    "    y = df_prem_no_outlier[PREDICTED_COL]\n",
    "    \n",
    "    # Predicting coverage with winscore\n",
    "#     all_col = copy.deepcopy(df_prem.columns)\n",
    "#     y_col = 'Coverage Premium_l'\n",
    "#     X_col = all_col.drop('Coverage Premium_l')\n",
    "\n",
    "#     X_lr = df_prem[X_col]\n",
    "#     y_lr = df_prem[y_col]\n",
    "\n",
    "#     #Encode status column\n",
    "#     status = X_lr['status']\n",
    "#     enc = dt.generate_encoder(status)\n",
    "    \n",
    "#     pickle.dump( enc, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\status.pkl', \"wb\" ) )\n",
    "#     status_enc = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\encoder\\status.pkl', \"rb\" ) )\n",
    "    \n",
    "    \n",
    "#     X_lr['status'] = dt.encode(status,status_enc)\n",
    "\n",
    "#     x = X_lr.values #returns a numpy array\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     x_scaled = min_max_scaler.fit_transform(x)\n",
    "#     X_lr = pandas.DataFrame(x_scaled,columns = X_lr.columns.values)\n",
    "\n",
    "    def train_model(X,y,epochs=2):\n",
    "\n",
    "        input_dim = X.shape[1]\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(input_dim, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(32, input_dim=16, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "        # Compile model\n",
    "    #     sgd = SGD(lr=0.01,nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        model.fit(X, y, nb_epoch=epochs)\n",
    "        return model\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=100)\n",
    "\n",
    "#     model = train_model(X,y,epochs=2000)\n",
    "    \n",
    "#     pickle.dump( model, open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\model\\premium_advice.pkl', \"wb\" ) )\n",
    "#     model = pickle.load( open( r'C:\\Users\\shraddha.sharma\\Projects\\Premium advise\\pickle\\model\\premium_advice.pkl', \"rb\" ) )\n",
    "    \n",
    "#     y_pred = model.predict(X)\n",
    "    prem_adv_model = train_model(X,y,epochs=20)\n",
    "    \n",
    "#     # serialize model to JSON\n",
    "    model_json = prem_adv_model.to_json()\n",
    "    with open(sm.PKL_PATH + '\\premium_advice.json', \"w\" ) as json_file:\n",
    "        json_file.write(model_json)\n",
    "    prem_adv_model.save_weights(sm.PKL_PATH + '\\premium_advice_weights.h5')\n",
    "\n",
    "    json_file = open(sm.PKL_PATH + '\\premium_advice.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pa_model = model_from_json(loaded_model_json)\n",
    "    pa_model.load_weights(sm.PKL_PATH + '\\premium_advice_weights.h5')\n",
    "    y_pred = pa_model.predict(X)    \n",
    "    result = pd.concat([X.reset_index(drop=True)\n",
    "           ,y.reset_index(drop=True)\n",
    "           ,pd.DataFrame([i[0] for i in y_pred],columns=['Predicted'])]\n",
    "          ,axis=1)\n",
    "    \n",
    "    logging.debug('debug: ' + strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()))\n",
    "    logging.info('info: ' + strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()))\n",
    "    logging.warning('warning: ' + strftime(\"%a, %d %b %Y %H:%M:%S +0000\", localtime()))\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = {\"Line of Business\":\"Excess & Umbrella\",\n",
    "\"BusinessClassification\":\"unknown\",\n",
    "\"InsuredState\":\"NJ\",\n",
    "\"InceptionDate\":\"7/1/2015\",\n",
    "\"ExpiryDate\":\"7/1/2016\",\n",
    "\"IsAdmitted\":1,\n",
    "\"Type\":\"Renewal\",\n",
    "\"status\":\"Issued\",\n",
    "\"Lead Excess-All Other_c\":0,\n",
    "\"Lead Excess-Real Estate/Property Mgmt/Office Building_c\":0,\n",
    "\"Lead Excess-Contracting/Construction_c\":0,\n",
    "\"Excess-Real Estate/Property Mgmt/Office Building_c\":0,\n",
    "\"Excess-Manufacturing_c\":0,\n",
    "\"Lead Excess-Retail/Wholesale_c\":0,\n",
    "\"Lead Excess-Municipalities/Hospitals/Political_c\":0,\n",
    "\"Lead Excess-Installation/Service or Repair_c\":0,\n",
    "\"Excess-Installation/Service or Repair_c\":0,\n",
    "\"Excess-Entertainment/Recreation_c\":0,\n",
    "\"Excess-Retail/Wholesale_c\":0,\n",
    "\"Excess-Entertainment/Recreation_l\":0,\n",
    "\"Excess-Municipalities/Hospitals/Political_c\":1,\n",
    "\"Lead Excess-Manufacturing_c\":0,\n",
    "\"Excess-Manufacturing_l\":0,\n",
    "\"Excess-Contracting/Construction_c\":0,\n",
    "\"Lead Excess-All Other_l\":0,\n",
    "\"Lead Excess-Real Estate/Property Mgmt/Office Building_l\":0,\n",
    "\"Excess-All Other_c\":0,\n",
    "\"Umbrella-Contracting/Construction\":0,\n",
    "\"Lead Excess-Entertainment/Recreation_c\":0,\n",
    "\"Excess-Contracting/Construction_l\":0,\n",
    "\"Excess-Installation/Service or Repair_l\":0,\n",
    "\"Lead Excess-Installation/Service or Repair_l\":0,\n",
    "\"Excess-Real Estate/Property Mgmt/Office Building_l\":0,\n",
    "\"Excess-All Other_l\":0,\n",
    "\"Excess-Retail/Wholesale_l\":0,\n",
    "\"Umbrella-All Other\":0,\n",
    "\"Lead Excess-Contracting/Construction_l\":0,\n",
    "\"Lead Excess-Entertainment/Recreation_l\":0,\n",
    "\"Lead Excess-Municipalities/Hospitals/Political_l\":0,\n",
    "\"Lead Excess-Manufacturing_l\":0,\n",
    "\"Excess-Municipalities/Hospitals/Political_l\":0,\n",
    "\"Lead Excess-Retail/Wholesale_l\":0,\n",
    "\"Total Layer Limit_c\":1,\n",
    "\"General Liability Allocation_c\":1,\n",
    "\"Coverage Premium_c\":1,\n",
    "\"Uninsured/Under Insured Motorist Premium_c\":0,\n",
    "\"Attachment Amount_c\":1,\n",
    "\"Per Occurrence limit_c\":1,\n",
    "\"Layer Aggregate_c\":1,\n",
    "\"Terrorism_c\":1,\n",
    "\"Aggregate_c\":1,\n",
    "\"Auto Liability Allocation_c\":1,\n",
    "\"Total Layer Limit_l\":6000000,\n",
    "\"General Liability Allocation_l\":1866,\n",
    "\"Coverage Premium_l\":19658,\n",
    "\"Uninsured/Under Insured Motorist Premium_l\":0,\n",
    "\"Attachment Amount_l\":15000000,\n",
    "\"Per Occurrence limit_l\":6000000,\n",
    "\"Layer Aggregate_l\":6000000,\n",
    "\"Terrorism_l\":3034,\n",
    "\"Aggregate_l\":6000000,\n",
    "\"Auto Liability Allocation_l\":17724}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle,copy\n",
    "import sys\n",
    "from AnalysisModule import data_analysis\n",
    "from TransformationModule import data_transformation\n",
    "import SettingsModule as sm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def main_1(var):\n",
    "\n",
    "    df = pd.DataFrame([var]) # convert dict to dataframe\n",
    "    da = data_analysis()\n",
    "    da.set_data(df)\n",
    "    datatypes_json = sm.DATATYPE\n",
    "    split_data = da.split_data_on_datatypes(datatypes_json)\n",
    "    \n",
    "    dt = data_transformation()\n",
    "    for each in split_data['numerical']:\n",
    "        y = 0\n",
    "        split_data['numerical'][each] = split_data['numerical'][each].apply(lambda x: dt.impute(x,y))   \n",
    "    for each in split_data['categorical']:\n",
    "        split_data['categorical'][each] = split_data['categorical'][each].apply(lambda x: dt.impute(x,'unknown'))\n",
    "\n",
    "    features = ['Layer Aggregate_l', 'Total Layer Limit_l', 'Per Occurrence limit_l','Aggregate_l']\n",
    "\n",
    "    # Separating out the features\n",
    "    df = split_data['numerical']\n",
    "    x = df[features]\n",
    "\n",
    "    # Standardizing the features\n",
    "    std_scale = pickle.load( open( sm.ENC_PATH + '\\std_scaler.pkl', \"rb\" ) )\n",
    "    x = std_scale.transform(x)\n",
    "\n",
    "    # PCA    \n",
    "    pc = pickle.load( open( sm.ENC_PATH + '\\pca.pkl', \"rb\" ) )\n",
    "    principalComponent = pc.transform(x)\n",
    "    principalDf = pd.DataFrame(data = principalComponent, columns = ['principal component 1'])\n",
    "    \n",
    "    # merging PCA and other numerical columns\n",
    "    df.drop(features, axis=1, inplace=True)\n",
    "    num_data = pd.concat([principalDf,df],axis=1)\n",
    "    \n",
    "    y_cat =  split_data['categorical']['status']\n",
    "\n",
    "    ## label encoding categorical variable\n",
    "    df =pd.DataFrame()\n",
    "    for each in split_data['categorical'].columns:\n",
    "        encoder = pickle.load( open( sm.ENC_PATH + '\\encoder_' +  str(each.replace('/','_')) +'.pkl', \"rb\" ) )\n",
    "        enc = encoder.transform(split_data['categorical'][each])\n",
    "        df = pd.concat([df,pd.DataFrame(enc)],axis=1)\n",
    "    df.columns = split_data['categorical'].columns\n",
    "    y_num = df['status']\n",
    "    df.drop('Line of Business',axis = 1,inplace = True) # removing Line of Business because there is only one Line of Business\n",
    "                \n",
    "    # Win score\n",
    "\n",
    "    def f1(x,y):\n",
    "        x = dt.datetime_transformation(x)\n",
    "        y = dt.datetime_transformation(y)\n",
    "        return (x-y).days\n",
    "\n",
    "    duration = split_data['datetime'].apply(lambda x:f1(x['ExpiryDate'],x['InceptionDate']),axis=1)\n",
    "    \n",
    "    cols = list(num_data.columns.values)\n",
    "    cols.extend(split_data['categorical'].columns.values)\n",
    "    cols.remove('Line of Business')\n",
    "    cols.remove('status')\n",
    "    cols.append('duration')\n",
    "    \n",
    "    df1 = pd.concat([num_data,df,duration,y_num],axis = 1)\n",
    "    df1['status_cat'] = y_cat\n",
    "    \n",
    "    X = df1[df1['Coverage Premium_l'] > 0]\n",
    "    y = X['status_cat']\n",
    "    X = X.drop('status',axis=1)\n",
    "    X = X.drop('status_cat',axis=1)\n",
    "    X.columns = cols\n",
    "\n",
    "    # feature importance and top imp features\n",
    "    imp_col = ['principal component 1', 'General Liability Allocation_l', 'Coverage Premium_l', 'Attachment Amount_l'\n",
    "               , 'Terrorism_l', 'Auto Liability Allocation_l', 'BusinessClassification', 'InsuredState', 'duration']\n",
    "    model = pickle.load( open( sm.PKL_PATH + '\\model.pkl', \"rb\" ) )\n",
    "    y_pred_prob = pd.DataFrame(model.predict_proba(X[imp_col]),columns = model.classes_)    \n",
    "    y_pred_prob['Score'] = dt.get_score(y_pred_prob)\n",
    "        \n",
    "    df_prem = pd.concat([df,num_data,duration,y_pred_prob['Score']],axis = 1)\n",
    "    \n",
    "    PREDICTED_COL = 'Coverage Premium_l'\n",
    "\n",
    "    X_col =  copy.deepcopy(list(df_prem.columns.values))\n",
    "    X_col.remove(PREDICTED_COL)\n",
    "    X = df_prem[X_col]\n",
    "    y = df_prem[PREDICTED_COL]\n",
    "    \n",
    "    ## Predicting coverage\n",
    "\n",
    "#     prem_adv_model = train_model(X,y,epochs=20)\n",
    "    \n",
    "#     # serialize model to JSON\n",
    "#     model_json = prem_adv_model.to_json()\n",
    "#     with open(sm.PKL_PATH + '\\premium_advice.json', \"w\" ) as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     prem_adv_model.save_weights(sm.PKL_PATH + '\\premium_advice_weights.h5')\n",
    "\n",
    "    json_file = open(sm.PKL_PATH + '\\premium_advice.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    pa_model = model_from_json(loaded_model_json)\n",
    "    pa_model.load_weights(sm.PKL_PATH + '\\premium_advice_weights.h5')\n",
    "    y_pred = pa_model.predict(X)    \n",
    "    return y_pred[0][0]\n",
    "\n",
    "main_1(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
